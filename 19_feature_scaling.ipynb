{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f85463c-c807-4629-8c1f-1f650b6f23c7",
   "metadata": {},
   "source": [
    "Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its application.\n",
    "\n",
    "Answer--> Min-Max scaling, also known as normalization, is a data preprocessing technique used to rescale numerical features to a fixed range, typically between 0 and 1. \n",
    "\n",
    "Here's an example to illustrate the application of Min-Max scaling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89c4c207-b505-4767-a436-eaa5112bdf8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.   0.25 0.5  0.75 1.  ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Raw data\n",
    "data = np.array([20, 30, 40, 50, 60])\n",
    "\n",
    "# Calculate the minimum and maximum values\n",
    "min_val = np.min(data)\n",
    "max_val = np.max(data)\n",
    "\n",
    "# Apply Min-Max scaling\n",
    "scaled_data = (data - min_val) / (max_val - min_val)\n",
    "\n",
    "# Print the scaled data\n",
    "print(scaled_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cedad38-e6b9-4a77-aa56-0d60de9fbf59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "15b468a3-7a76-4935-955e-973ae0a41dbd",
   "metadata": {},
   "source": [
    "Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling? Provide an example to illustrate its application.\n",
    "\n",
    "Answer--> The Unit Vector technique, also known as unit normalization is a data preprocessing technique used to rescale numerical features to have a unit norm. In this technique, each feature vector is divided by its magnitude, resulting in a vector with a magnitude of 1.\n",
    "\n",
    "Here's an example to illustrate the application of the Unit Vector technique:\n",
    "\n",
    "Consider a dataset with two features representing the height (in centimeters) and weight (in kilograms) of individuals:\n",
    "\n",
    "```\n",
    "Height: [160, 170, 180]\n",
    "Weight: [50, 60, 70]\n",
    "```\n",
    "\n",
    "To apply the Unit Vector technique, we follow these steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36a70fc8-5901-4622-aba2-0e98b02c97b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unit vector of Height [[1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Unit vector of Weight [[1.]\n",
      " [1.]\n",
      " [1.]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "df = pd.DataFrame({\n",
    "    \"Height\": [150, 170, 180],\n",
    "    \"Weight\": [50, 60, 70]\n",
    "})\n",
    "\n",
    "from sklearn.preprocessing import normalize \n",
    "\n",
    "normalize_height = normalize(df[[\"Height\"]])\n",
    "normalize_weight = normalize(df[[\"Weight\"]])\n",
    "\n",
    "print (\"Unit vector of Height\", normalize_height)\n",
    "print (\"Unit vector of Weight\", normalize_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065feb55-5282-4c6d-bb27-a6ca1fc09d77",
   "metadata": {},
   "source": [
    "The resulting vectors have unit norms, meaning their magnitudes are 1. This normalization technique is useful when the magnitude of the features is significant, and we want to emphasize the direction or orientation of the vectors rather than their absolute values.\n",
    "\n",
    "Differences between Unit Vector technique and Min-Max scaling:\n",
    "- Min-Max scaling rescales features to a fixed range, while the Unit Vector technique scales vectors to have unit norms.\n",
    "- Min-Max scaling preserves relative relationships and proportions within the range, while the Unit Vector technique focuses on vector direction or orientation.\n",
    "- Min-Max scaling is feature-wise, while the Unit Vector technique is vector-wise.\n",
    "- Min-Max scaling is used when features have different scales or units, while the Unit Vector technique emphasizes vector directionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a0e919-1e00-4753-936e-cac54ba81de0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea74341-bb99-41aa-b31c-52a4e92dfa05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7735b161-6f99-4ce2-847a-f2e1e61756d0",
   "metadata": {},
   "source": [
    "Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an example to illustrate its application.\n",
    "\n",
    "Answer--> PCA (Principal Component Analysis) is a statistical technique used for dimensionality reduction. It aims to transform a high-dimensional dataset into a lower-dimensional space while preserving the maximum amount of information or variance in the data. It achieves this by finding the principal components, which are new orthogonal axes that capture the most significant variations in the data.  \n",
    "\n",
    "Let's consider an example to illustrate the application of PCA for dimensionality reduction:\n",
    "\n",
    "Suppose we have a dataset with four features: \"height,\" \"weight,\" \"age,\" and \"income\" of individuals. The dataset contains information for 1000 individuals.\n",
    "\n",
    "To apply PCA for dimensionality reduction, we follow these steps:\n",
    "\n",
    "1. Standardize the data: Since PCA is sensitive to the scale of the features, it is often necessary to standardize the data by subtracting the mean and dividing by the standard deviation. This step ensures that features with larger scales do not dominate the analysis.\n",
    "\n",
    "2. Calculate the covariance matrix: The covariance matrix measures the relationships between pairs of features. It helps in understanding the variability and correlation present in the data.\n",
    "\n",
    "3. Compute the eigenvectors and eigenvalues: The eigenvectors represent the principal components, and the corresponding eigenvalues indicate the amount of variance explained by each principal component. The eigenvectors are computed from the covariance matrix.\n",
    "\n",
    "4. Select the desired number of principal components: Based on the eigenvalues, we can determine the number of principal components we want to retain. These components should capture a significant portion of the total variance in the data.\n",
    "\n",
    "5. Project the data onto the selected principal components: The data is transformed by projecting it onto the selected principal components. This projection reduces the dimensionality of the data while retaining as much information or variance as possible.\n",
    "\n",
    "For example, let's say after performing PCA, we decide to keep only the first two principal components, which explain most of the variance in the data.\n",
    "\n",
    "The transformed dataset will now have two features (the first two principal components) instead of the original four features.\n",
    "\n",
    "By reducing the dimensionality using PCA, we can visualize the data in a lower-dimensional space, simplify subsequent analysis tasks, and potentially improve computational efficiency. The retained principal components capture the most significant variations in the data, allowing us to focus on the most important aspects while discarding less informative features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c819935-f238-45e7-b629-5e309b33664a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f49a37d-b74e-40fa-98c5-4dd21abe24be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "606e8f50-ef8b-4709-9413-9170fe10f053",
   "metadata": {},
   "source": [
    "Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to preprocess the data.\n",
    "\n",
    "Answer--> To preprocess the data for building a recommendation system for a food delivery service, Min-Max scaling can be used to normalize the features such as price, rating, and delivery time. Here's how Min-Max scaling can be applied to each feature:\n",
    "\n",
    "1. Determine the minimum and maximum values of each feature in the dataset.\n",
    "\n",
    "For example:\n",
    "   - Price: Minimum price of food items could be 5 doller , and the maximum price could be 50 doller.\n",
    "   - Rating: The minimum rating could be 1.0, and the maximum rating could be 5.0.\n",
    "   - Delivery time: The minimum delivery time could be 15 minutes, and the maximum delivery time could be 60 minutes.\n",
    "\n",
    "2. Apply the Min-Max scaling formula to each feature:\n",
    "   - Scaled value = (value - minimum) / (maximum - minimum)\n",
    "\n",
    "   For instance:\n",
    "   - Scaled Price = (Price - 5) / (50 - 5)\n",
    "   - Scaled Rating = (Rating - 1.0) / (5.0 - 1.0)\n",
    "   - Scaled Delivery time = (Delivery time - 15) / (60 - 15)\n",
    "\n",
    "   The scaled values will now range between 0 and 1.\n",
    "\n",
    "3. After scaling, the data will be ready to be used in the recommendation system. The scaled values ensure that each feature is proportionally transformed within the desired range. This step is particularly useful in scenarios where the features have different scales or units.\n",
    "\n",
    "By applying Min-Max scaling, the features such as price, rating, and delivery time will be normalized to a common scale, allowing them to be compared and analyzed together. This preprocessing step helps avoid dominance by features with larger values and ensures that the features maintain their relative relationships within the transformed range. It facilitates the recommendation system in considering and weighing each feature appropriately when suggesting food items to users based on their preferences. Create a new DataFrame with the scaled features\n",
    "scaled_data = pd.DataFrame(scaled_features, columns=features.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fb27f1-4ec8-43fb-92f4-c07036ef448b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69088a2-0aa2-4013-8f6c-2f86c21e504e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bfef0de9-88c8-458e-959a-75ec8824e813",
   "metadata": {},
   "source": [
    "Q6. You are working on a project to build a model to predict stock prices. The dataset contains many features, such as company financial data and market trends. Explain how you would use PCA to reduce the dimensionality of the dataset.\n",
    "\n",
    "Answer--> Here's how PCA can be applied to achieve dimensionality reduction:\n",
    "\n",
    "1. Data preprocessing: Before applying PCA, it's crucial to preprocess the dataset by standardizing the features. Standardization involves subtracting the mean and dividing by the standard deviation of each feature. This step ensures that all features are on a comparable scale, which is essential for PCA to work effectively.\n",
    "\n",
    "2. Covariance matrix calculation: Compute the covariance matrix using the standardized dataset. The covariance matrix measures the relationships and variances between pairs of features. It provides crucial information for determining the principal components.\n",
    "\n",
    "3. Eigenvalue decomposition: Perform eigenvalue decomposition on the covariance matrix to obtain the eigenvectors and eigenvalues. The eigenvectors represent the principal components, while the eigenvalues signify the amount of variance explained by each principal component.\n",
    "\n",
    "4. Selection of principal components: Determine the number of principal components to retain based on the eigenvalues. You can choose a threshold, such as retaining components that explain a certain percentage of the total variance (e.g., 95% variance explained). Alternatively, you can decide to keep a fixed number of components that are most relevant to the problem.\n",
    "\n",
    "5. Projection of data: Project the original dataset onto the selected principal components to obtain a reduced-dimensional representation. Each instance in the dataset is transformed into a new set of values corresponding to the retained principal components.\n",
    "\n",
    "By applying PCA for dimensionality reduction, you effectively reduce the number of features in the dataset while capturing the most critical information and retaining the maximum amount of variance. This reduction in dimensionality can simplify the dataset, enhance interpretability, and potentially improve the performance of the stock price prediction model. Additionally, it helps mitigate the curse of dimensionality, which can lead to overfitting and computational inefficiency when dealing with high-dimensional datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77a3e0c-2ada-4b32-99f7-d21a9409f419",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6baeb0-bbe5-4aae-8b9e-943a106242a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8660c97c-4a4b-48c4-bb42-d599a3cd1213",
   "metadata": {},
   "source": [
    "Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the values to a range of -1 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84dadd9b-3aa5-4fb1-b521-64eb76744c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.9999999999999999, -0.5789473684210525, -0.05263157894736836, 0.47368421052631593, 1.0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Original dataset\n",
    "data = [1, 5, 10, 15, 20]\n",
    "\n",
    "# Create an instance of MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "# Reshape the data to 2D array (required by MinMaxScaler)\n",
    "data = [[value] for value in data]\n",
    "\n",
    "# Perform Min-Max scaling\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "# Flatten the scaled data back to 1D array\n",
    "scaled_data = [value[0] for value in scaled_data]\n",
    "\n",
    "# Print the scaled data\n",
    "print(scaled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf3c14f-f227-4d47-a78a-26db47f0e554",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154563fc-3deb-41d1-b53c-0cfa11c6c739",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8562fdb7-0dc7-4765-8b83-1baf9a69323c",
   "metadata": {},
   "source": [
    "Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform Feature Extraction using PCA. How many principal components would you choose to retain, and why?\n",
    "\n",
    "Answer--> Here's an approach to determine the number of principal components to retain:\n",
    "\n",
    "- Standardize the data: Before applying PCA, it is generally recommended to standardize the data by subtracting the mean and scaling the features to have unit variance. This step ensures that features with larger scales do not dominate the analysis.\n",
    "\n",
    "- Perform PCA: Apply PCA to the standardized dataset. The result will provide the principal components and their associated eigenvalues.\n",
    "\n",
    "- Compute explained variance ratio: Calculate the explained variance ratio for each principal component. This can be done by dividing each eigenvalue by the sum of all eigenvalues.\n",
    "\n",
    "- Calculate cumulative explained variance ratio: Compute the cumulative sum of the explained variance ratios.\n",
    "\n",
    " Determine the number of PCs to retain: Examine the cumulative explained variance ratio plot. Look for the point where adding additional PCs provides diminishing returns in terms of explained variance. This can be subjective and depends on the specific dataset and the desired level of information retention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de30d5bd-69f8-4326-b4cf-1a71a05bf989",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
